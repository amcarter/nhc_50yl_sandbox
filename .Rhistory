site == "cbp" ~ "CBP",
site == "pm" ~ "PM",
site == "wb" ~ "WB",
site == "wbp" ~ "WBP",
site == "unhc" ~ "UNHC")) %>%
group_by(site, year, month) %>%
summarize(across(.fns = list(mean = ~mean(.,na.rm = T),
sd = ~sd(.,na.rm = T)),
.names = "{col}_{fn}")) %>%
ungroup()
sum_month <- month_preds %>%
left_join(site_dat, by = "site")
site_dat
hpreds <- hmet$preds %>%
as_tibble() %>%
select(-K600, -good_flow, -depth) %>%
mutate(month = substr(date, 6, 7),
across(starts_with(c("GPP","ER")), ~ . * O2toC))
hmonth_preds <- hpreds %>%
select(-date) %>%
group_by(site, year, month) %>%
summarize(across(.fns = list(mean = ~mean(.,na.rm = T),
sd = ~sd(.,na.rm = T)),
.names = "{col}_{fn}")) %>%
ungroup()
hmonth_preds <- hpreds %>%
select(-date, -year, -site) %>%
group_by(month) %>%
summarize(across(.fns = list(mean = ~mean(.,na.rm = T),
sd = ~sd(.,na.rm = T)),
.names = "{col}_{fn}")) %>%
ungroup() %>%
mutate(site = "all") %>%
bind_rows(hmonth_preds)
hmonth_preds <- hpreds %>%
filter(site == "CBP") %>%
select(-date, -year, -site) %>%
group_by(month) %>%
summarize(across(.fns = list(mean = ~mean(.,na.rm = T),
sd = ~sd(.,na.rm = T)),
.names = "{col}_{fn}")) %>%
ungroup() %>%
mutate(site = "CBP") %>%
bind_rows(hmonth_preds)
hall_month <- hall %>%
mutate(month = substr(date, 6, 7),
year = as.numeric(substr(date, 1, 4))) %>%
left_join(hallQT[,c(1,2,4)], by = "date") %>%
select(-date) %>%
group_by(site, year, month) %>%
summarize(across(.fns = list(mean = ~mean(.,na.rm = T),
sd = ~sd(.,na.rm = T)),
.names = "{col}_{fn}")) %>%
ungroup()
hall_month <- hall %>%
mutate(month = substr(date, 6, 7)) %>%
left_join(hallQT[,c(1,2,4)], by = "date") %>%
select(-date, -site) %>%
group_by(month) %>%
summarize(across(.fns = list(mean = ~mean(.,na.rm = T),
sd = ~sd(.,na.rm = T)),
.names = "{col}_{fn}")) %>%
ungroup() %>%
mutate(site = "all") %>%
bind_rows(hall_month)
hall_month <- hall %>%
filter(site == "CBP") %>%
mutate(month = substr(date, 6, 7)) %>%
left_join(hallQT[,c(1,2,4)], by = "date") %>%
select(-date, -site) %>%
group_by(month) %>%
summarize(across(.fns = list(mean = ~mean(.,na.rm = T),
sd = ~sd(.,na.rm = T)),
.names = "{col}_{fn}")) %>%
ungroup() %>%
mutate(site = "CBP") %>%
bind_rows(hall_month)
ggplot(comp, aes(distance_m, gpp_cum))+
geom_line() +
geom_line(aes(y = er_cum)) +
geom_line(aes(y = gpp_cum + er_cum), lty = 2) +
facet_grid(group~.) +
labs(title = "cumulative metabolism along 10 km",
y = "GPP, ER, and NEP (gC/m2/y)")
ggplot(sum_month, aes(x = as.numeric(month), y = GPP_mean, col = width_mar_m)) +
geom_point(size = 2)
month_preds <- month_preds %>%
mutate(month = as.numeric(month))
hall_month <- hall_month %>%
mutate(month = as.numeric(month))
allm <- month_preds %>%
filter(site != "all")%>%
group_by(site, month) %>%
summarize(across(-year, mean, na.rm = T)) %>%
ungroup()
all <- allm %>%
group_by(month) %>%
summarize(across(-site, mean, na.rm = T)) %>%
ungroup() %>%
mutate(site = "all")
allm <- month_preds
ggplot(allm, aes(x = as.numeric(month), y = GPP_mean)) +
geom_line() +
geom_ribbon(aes(ymin = GPP_mean - GPP_sd,
ymax = GPP_mean + GPP_sd),
fill = alpha("forestgreen", .3), col = NA) +
geom_line(aes(y = ER_mean)) +
geom_ribbon(aes(ymin = ER_mean - ER_sd,
ymax = ER_mean + ER_sd),
fill = alpha("sienna", .3), col = NA) +
facet_wrap(.~ site) +
ylim(-4,1.2) +
geom_hline(yintercept = 0, col = "grey") +
labs(title = "2019 monthly average metabolism from StreamMetabolizer",
x = "month", y = "gC/m2/d")
allm
allm %>% filter(site %in% c("NHC", "UNHC"))
allm %>% filter(site %in% c("NHC", "UNHC")) %>%
ggplot( aes(x = as.numeric(month), y = GPP_mean)) +
geom_line() +
geom_ribbon(aes(ymin = GPP_mean - GPP_sd,
ymax = GPP_mean + GPP_sd),
fill = alpha("forestgreen", .3), col = NA) +
geom_line(aes(y = ER_mean)) +
geom_ribbon(aes(ymin = ER_mean - ER_sd,
ymax = ER_mean + ER_sd),
fill = alpha("sienna", .3), col = NA) +
facet_wrap(.~ year) +
ylim(-4,1.2) +
geom_hline(yintercept = 0, col = "grey") +
labs(title = "Interannual monthly average metabolism from StreamMetabolizer",
x = "month", y = "gC/m2/d")
allm
allm %>% filter(site %in% c("NHC", "UNHC"))
12*3*2
allm %>% filter(site %in% c("NHC", "UNHC")) %>%
ggplot( aes(x = as.numeric(month), y = GPP_mean)) +
geom_line() +
geom_ribbon(aes(ymin = GPP_mean - GPP_sd,
ymax = GPP_mean + GPP_sd),
fill = alpha("forestgreen", .3), col = NA) +
geom_line(aes(y = ER_mean)) +
geom_ribbon(aes(ymin = ER_mean - ER_sd,
ymax = ER_mean + ER_sd),
fill = alpha("sienna", .3), col = NA) +
facet_grid(site~ year) +
ylim(-4,1.2) +
geom_hline(yintercept = 0, col = "grey") +
labs(title = "Interannual monthly average metabolism from StreamMetabolizer",
x = "month", y = "gC/m2/d")
png("C:/Users/Alice Carter/Dropbox (Duke Bio_Ea)/projects/hall_50yl/figures/interannual_monthly_avg_met_all_sites_raymond.png",
width = 7.5, height = 5, res = 300, units = "in")
allm %>% filter(site %in% c("NHC", "UNHC")) %>%
ggplot( aes(x = as.numeric(month), y = GPP_mean)) +
geom_line() +
geom_ribbon(aes(ymin = GPP_mean - GPP_sd,
ymax = GPP_mean + GPP_sd),
fill = alpha("forestgreen", .3), col = NA) +
geom_line(aes(y = ER_mean)) +
geom_ribbon(aes(ymin = ER_mean - ER_sd,
ymax = ER_mean + ER_sd),
fill = alpha("sienna", .3), col = NA) +
facet_grid(site~ year) +
ylim(-4,1.2) +
geom_hline(yintercept = 0, col = "grey") +
labs(title = "Interannual monthly average metabolism from StreamMetabolizer",
x = "month", y = "gC/m2/d")
dev.off()
ggplot(allm, aes(x = as.numeric(month), y = -GPP_mean/ER_mean)) +
geom_line() +
facet_wrap(.~ site, scales = "free_y") +
geom_hline(yintercept = 1, col = "grey30", lty = 2) +
labs(title = "2019 Monthly Productivity:Respiration from StreamMetabolizer",
x = "month", y = "GPP/ER")
allm %>% filter(site %in% c("NHC", "UNHC")) %>%
ggplot(allm, aes(x = as.numeric(month), y = -GPP_mean/ER_mean)) +
geom_line() +
facet_grid(site~year, scales = "free_y") +
geom_hline(yintercept = 1, col = "grey30", lty = 2) +
labs(title = "2019 Monthly Productivity:Respiration from StreamMetabolizer",
x = "month", y = "GPP/ER")
allm %>% filter(site %in% c("NHC", "UNHC")) %>%
ggplot(aes(x = as.numeric(month), y = -GPP_mean/ER_mean)) +
geom_line() +
facet_grid(site~year, scales = "free_y") +
geom_hline(yintercept = 1, col = "grey30", lty = 2) +
labs(title = "2019 Monthly Productivity:Respiration from StreamMetabolizer",
x = "month", y = "GPP/ER")
png("C:/Users/Alice Carter/Dropbox (Duke Bio_Ea)/projects/hall_50yl/figures/interannual_monthly_PR_all_sites_raymond.png",
width = 7.5, height = 5, res = 300, units = "in")
allm %>% filter(site %in% c("NHC", "UNHC")) %>%
ggplot(aes(x = as.numeric(month), y = -GPP_mean/ER_mean)) +
geom_line() +
facet_grid(site~year, scales = "free_y") +
geom_hline(yintercept = 1, col = "grey30", lty = 2) +
labs(title = "2019 Monthly Productivity:Respiration from StreamMetabolizer",
x = "month", y = "GPP/ER")
dev.off()
library(StreamPULSE)
library(viridis)
library(ggplot2)
library(beanplot)
library(scales)
library(tidyverse)
library(lubridate)
setwd('C:/Users/Alice Carter/Dropbox (Duke Bio_Ea)/projects/hall_50yl/code')
#switch this to TRUE if you want to use only modern metab estimates from days
#in which Q is in the same ballpark as it was for HALL's K estimates
filter_high_Q = FALSE
#read in historic data and average across multiple same-site, same-day estimates
nhc_68_70 = read.csv('data/hall_data/hall_table_15.csv', colClasses=c('date'='Date'))
nhc_68_70 = nhc_68_70 %>%
group_by(date, site) %>%
summarize_if(is.numeric, mean, na.rm=TRUE) %>%
as.data.frame()
# should the high value from the storm be included?
nhc_68_70 <- nhc_68_70 %>%
mutate(GPP_gO2m2d = ifelse(GPP_gO2m2d > 6, NA, GPP_gO2m2d),
ER_gO2m2d = ifelse(ER_gO2m2d > 10, NA, ER_gO2m2d))
nhc_68 <- nhc_68_70 %>%
filter(date < nhc_68_70$date[1] + 365)
nhc_69 <- nhc_68_70 %>%
filter(date < nhc_68_70$date[1] + 2*365,
date >= nhc_68_70$date[1] + 365)
#subset historic data by site and year
gpp_concrete = nhc_68_70$GPP_gO2m2d[nhc_68_70$site == 'Concrete']
gpp_blackwood = nhc_68_70$GPP_gO2m2d[nhc_68_70$site == 'Blackwood']
gpp_wb = nhc_68_70$GPP_gO2m2d[nhc_68_70$site == 'Wood Bridge']
gpp_68_70 = nhc_68_70$GPP_gO2m2d
gpp_68 = nhc_68$GPP_gO2m2d
gpp_69 = nhc_69$GPP_gO2m2d
er_68_70 = nhc_68_70$ER_gO2m2d
er_68 = nhc_68$ER_gO2m2d
er_69 = nhc_69$ER_gO2m2d
nep_68_70 = gpp_68_70 - er_68_70
nep_68 = gpp_68 - er_68
nep_69 = gpp_69 - er_69
# nhc_new <- read_csv("data/NHC_metab_allsites_fixedHallK_v2.csv") %>%
#     mutate(GPP = ifelse(GPP > -5, GPP, NA),
#            ER = ifelse(ER > -20, ER, NA),
#            K600 = ifelse(K600 <= 0, NA, K600),
#            year = year(date))
nhc_new <- read_rds("C:/Users/Alice Carter/Dropbox (Duke Bio_Ea)/projects/NHC_2019_metabolism/data/metabolism/hall/hall_met_60min.rds")$preds
if(filter_high_Q){
dc <- 0.65 # depth cutoff from Hall
#highest considered depth in Hall dissertation: 0.65m
#corresponding Q, based on modern Z-Q curve:
Q_cutoff = max(na.omit(nhc_new$discharge[nhc_new$depth > dc - 0.01 & nhc_new$depth < dc]))
Qbool = nhc_new$discharge < Q_cutoff
Qbool[is.na(Qbool)] = FALSE
nhc_new = nhc_new[Qbool,]
}
gpp_new = nhc_new$GPP
gpp_17 = nhc_new$GPP[nhc_new$year == 2017]
gpp_18 = nhc_new$GPP[nhc_new$year == 2018]
gpp_19 = nhc_new$GPP[nhc_new$year == 2019]
er_new = nhc_new$ER
er_17 = nhc_new$ER[nhc_new$year == 2017]
er_18 = nhc_new$ER[nhc_new$year == 2018]
er_19 = nhc_new$ER[nhc_new$year == 2019]
nep_new = gpp_new + er_new
nep_17 = gpp_17 + er_17
nep_18 = gpp_18 + er_18
nep_19 = gpp_19 + er_19
dates_new = nhc_new$date
#time-series comparison of means then and now? ####
plot(gpp_new, type='l')
acf(gpp_new, na.action=na.pass)
pacf(gpp_new, na.action=na.pass)
historic_dates = nhc_68_70$date[! is.na(nhc_68_70$GPP_gO2m2d)]
historic_year_agg = as.character(historic_dates)
substr(historic_year_agg, 1, 4) = '1970'
historic_year_agg = as.Date(historic_year_agg)
hy_num = as.numeric(historic_year_agg)
hd_concrete = nhc_68_70$date[! is.na(nhc_68_70$GPP_gO2m2d) &
nhc_68_70$site == 'Concrete']
concrete_year_agg = as.character(hd_concrete)
substr(concrete_year_agg, 1, 4) = '1970'
concrete_year_agg = as.Date(concrete_year_agg)
concrete_num = as.numeric(concrete_year_agg)
hd_blackwood = nhc_68_70$date[! is.na(nhc_68_70$GPP_gO2m2d) &
nhc_68_70$site == 'Blackwood']
blackwood_year_agg = as.character(hd_blackwood)
substr(blackwood_year_agg, 1, 4) = '1970'
blackwood_year_agg = as.Date(blackwood_year_agg)
blackwood_num = as.numeric(blackwood_year_agg)
hd_wb = nhc_68_70$date[! is.na(nhc_68_70$GPP_gO2m2d) &
nhc_68_70$site == 'Wood Bridge']
wb_year_agg = as.character(hd_wb)
substr(wb_year_agg, 1, 4) = '1970'
wb_year_agg = as.Date(wb_year_agg)
wb_num = as.numeric(wb_year_agg)
plot(historic_dates, rep(1, length(historic_dates), type='n', xlab='day'),
yaxt='n', ylab='', xlab='', main='Historic Coverage')
abline(v=historic_dates, lty=2, col='gray')
par(mfrow=c(4,1), mar=c(0,0,0,0), oma=c(3,4,3,0))
png(width=7, height=6, units='in', type='cairo', res=300,
filename='../figures/sitecoverage.png')
par(mfrow=c(4,1), mar=c(0,0,0,0), oma=c(3,4,3,0))
beanplot(hy_num, horizontal=TRUE, col='gray', xaxt='n',
frame.plot=FALSE, ylim=lims)
# main='Historic Annual Coverage Across Sites')
# axis(1, at=seq(as.Date('1970-01-01'), as.Date('1970-12-31'), length.out=13)[1:12],
#     labels=month.abb)
mtext('All sites', 2)
mtext('Historic Annual Coverage', 3)
legend('topright', legend=paste('n =', length(! is.na(hy_num))),
bty='n', cex=1.3, text.font=2)
#plot temporal coverage by site
lims = c(min(hy_num), max(hy_num))
beanplot(concrete_num, horizontal=TRUE, col='yellow', xaxt='n',
frame.plot=FALSE, ylim=lims)
mtext('Concrete', 2)
legend('topright', legend=paste('n =', length(! is.na(concrete_num))),
bty='n', cex=1.3, text.font=2)
beanplot(blackwood_num, horizontal=TRUE, col='green', xaxt='n',
frame.plot=FALSE, ylim=lims)
# legend('left', legend=c('Concrete', 'Blackwood', 'Wood Bridge'),
#     fill=c('yellow', 'green', 'orange'), cex=2, bty='n')
mtext('Blackwood', 2)
legend('topright', legend=paste('n =', length(! is.na(blackwood_num))),
bty='n', cex=1.3, text.font=2)
beanplot(wb_num, horizontal=TRUE, col='orange', xaxt='n',
frame.plot=FALSE, ylim=lims)
mtext('Wood Bridge', 2)
legend('topright', legend=paste('n =', length(! is.na(wb_num))),
bty='n', cex=1.3, text.font=2)
# ax_dt = as.numeric(historic_dates)
# ax_seq = seq(ax_dt[1], ax_dt[which.max(ax_dt)], length.out=10)
# axis(1, at=ax_seq, labels=as.Date(ax_seq), las=1, cex.axis=1.7)
axis(1, at=seq(as.Date('1970-01-01'), as.Date('1970-12-31'), length.out=13)[1:12],
labels=month.abb)
dev.off()
par(mfrow=c(2,2), mar=c(0, 0, 0, 0), oma=rep(4, 4))
qqnorm(gpp_new, lty=2, xlab='', ylab='', main='', xaxt='n', yaxt='n', bty='o')
abline(0, 1, col='red', lty=2)
legend('bottom', legend='GPP 2017-18', bty='n', cex=1.3)
qqnorm(er_new, lty=2, xlab='', ylab='', main='', xaxt='n', yaxt='n', bty='o')
abline(0, 1, col='red', lty=2)
legend('bottom', legend='ER 2017-18', bty='n', cex=1.3)
qqnorm(gpp_68_70, lty=2, xlab='', ylab='', main='', xaxt='n', yaxt='n', bty='o')
abline(0, 1, col='red', lty=2)
legend('top', legend='GPP 1968-70', bty='n', cex=1.3)
qqnorm(er_68_70, lty=2, xlab='', ylab='', main='', xaxt='n', yaxt='n', bty='o')
abline(0, 1, col='red', lty=2)
legend('top', legend='ER 1968-70', bty='n', cex=1.3)
mtext('Theoretical Quantiles', 1, outer=TRUE, line=1.5)
mtext('Sample Quantiles', 2, outer=TRUE, line=1.5)
mtext('Normal Q-Q Plots (red line = 1:1)', 3, outer=TRUE, line=1.5)
#nonnormal, but CLT probably applies.
#let's assess equality of variance with an F-test
var.test(gpp_68_70, gpp_new) #not equal: p < 0.001
var.test(er_68_70, er_new) #not equal: p < 0.00001
#get observed t-statistic
t_obs_gpp = t.test(gpp_68_70, gpp_new, var.equal=FALSE)$statistic
#artificially make both sample means identical (satisfy the null)
gpp_68_70_mod = gpp_68_70 - mean(gpp_68_70, na.rm=TRUE) +
mean(c(gpp_68_70, gpp_new), na.rm=TRUE)
gpp_new_mod = gpp_new - mean(gpp_new, na.rm=TRUE) +
mean(c(gpp_68_70, gpp_new), na.rm=TRUE)
#verify
round(mean(gpp_68_70_mod, na.rm=TRUE), 7) ==
round(mean(gpp_new_mod, na.rm=TRUE), 7)
#get historic monthly data coverage to use for sample weights
nhc_68_70 <- filter(nhc_68_70, !is.na(GPP_gO2m2d))
month_counts_68_70 = tapply(rep(1, nrow(nhc_68_70)),
substr(nhc_68_70$date, 6, 7), sum)
month_proportions = month_counts_68_70 / nrow(nhc_68_70)
#split gpp vector by month for each dataset
#commented portions are for uniformly distributing monthly draw weights
gpp_68_70_bymo <- split(gpp_68_70_mod[!is.na(gpp_68_70_mod)],
factor(substr(nhc_68_70$date, 6, 7)))
# gpp_68_70_bymo = split(gpp_68_70_mod,
#     factor(rep(c('01','02','03','04','05','06','07','08','10','11','12'),
#     length.out=length(gpp_68_70_mod))))
gpp_new_bymo = split(gpp_new_mod[!is.na(gpp_new_mod)],
factor(substr(dates_new[!is.na(gpp_new_mod)], 6, 7)))
# gpp_new_bymo = split(gpp_new_mod,
#     factor(rep(c('01','02','03','04','05','06','07','08','10','11','12'),
#     length.out=length(gpp_new_mod))))
nsamp_new = sum(sapply(gpp_new_bymo, length))
nsamp_68_70 = length(which(!is.na(gpp_68_70_mod)))
#determine monthly sample sizes for modern dataset; deal with remainders
month_samp_new = month_proportions * nsamp_new
extra_sample_probs = month_samp_new %% 1
month_samp_new = floor(month_samp_new)
#get bootstrap estimate of sampling distribution of the t-stat if H0 is true;
#i.e. bootstrap the null distribution (weight draws by historic monthly coverage)
nsamp = 20000
t_vect_gpp = vector(length=nsamp)
for(i in 1:nsamp){
samp_68_70_gpp = samp_new_gpp = c()
remainder_months = sample(c(1:8, 10:12), size=sum(extra_sample_probs),
prob=extra_sample_probs)
for(j in c(1:8, 10:12)){
extra_samp = ifelse(j %in% remainder_months, 1, 0)
j = sprintf('%02d', j)
samp_68_70_gpp = append(samp_68_70_gpp, sample(gpp_68_70_bymo[[j]],
size=month_counts_68_70[j], replace=TRUE))
samp_new_gpp = append(samp_new_gpp, sample(gpp_new_bymo[[j]],
size=month_samp_new[j] + extra_samp, replace=TRUE))
}
# samp_68_70_gpp = sample(gpp_68_70_mod, size=nsamp_68_70, replace=TRUE)
# samp_new_gpp = sample(gpp_new_mod, size=nsamp_new, replace=TRUE)
t_vect_gpp[i] = t.test(samp_68_70_gpp, samp_new_gpp,
var.equal=FALSE)$statistic
}
#p-val is proportion of times observed t-statistic >= bootstrap t-statistic
pval_gpp = (sum(t_vect_gpp <= t_obs_gpp) + 1) / (nsamp + 1)
if(pval_gpp == 1){
pval_gpp = (sum(t_vect_gpp >= t_obs_gpp) + 1) / (nsamp + 1)
}
#get observed t-statistic
t_obs_er = t.test(er_68_70, er_new, var.equal=FALSE)$statistic
#artificially make both sample means identical (satisfy the null)
er_68_70_mod = er_68_70 - mean(er_68_70, na.rm=TRUE) +
mean(c(er_68_70, er_new), na.rm=TRUE)
er_new_mod = er_new - mean(er_new, na.rm=TRUE) +
mean(c(er_68_70, er_new), na.rm=TRUE)
#verify
mean(er_68_70_mod, na.rm=TRUE) == mean(er_new_mod, na.rm=TRUE)
#split er vector by month for each dataset
er_68_70_bymo = split(er_68_70_mod[!is.na(er_68_70_mod)],
factor(substr(nhc_68_70$date, 6, 7)))
er_new_bymo = split(er_new_mod, factor(substr(dates_new, 6, 7)))
er_new_bymo = lapply(er_new_bymo, na.omit)
nsamp_new = sum(sapply(er_new_bymo, length))
nsamp_68_70 = length(er_68_70_mod[!is.na(er_68_70_mod)])
#determine monthly sample sizes for modern dataset; deal with remainders
month_samp_new = month_proportions * nsamp_new
extra_sample_probs = month_samp_new %% 1
month_samp_new = floor(month_samp_new)
#get bootstrap estimate of sampling distribution of the t-stat if H0 is true;
#i.e. bootstrap the null distribution (weight draws by historic monthly coverage)
nsamp = 20000
t_vect_er = vector(length=nsamp)
for(i in 1:nsamp){
samp_68_70_er = samp_new_er = c()
remainder_months = sample(c(1:8, 10:12), size=sum(extra_sample_probs),
prob=extra_sample_probs)
for(j in c(1:8, 10:12)){
extra_samp = ifelse(j %in% remainder_months, 1, 0)
j = sprintf('%02d', j)
samp_68_70_er = append(samp_68_70_er, sample(er_68_70_bymo[[j]],
size=month_counts_68_70[j], replace=TRUE))
samp_new_er = append(samp_new_er, sample(er_new_bymo[[j]],
size=month_samp_new[j] + extra_samp, replace=TRUE))
}
# samp_68_70_er = sample(er_68_70_mod, size=nsamp_68_70, replace=TRUE)
# samp_new_er = sample(er_new_mod, size=nsamp_new, replace=TRUE)
t_vect_er[i] = t.test(samp_68_70_er, samp_new_er,
var.equal=FALSE)$statistic
}
#p-val is proportion of times observed t-statistic >= bootstrap t-statistic
pval_er = (sum(t_vect_er >= t_obs_er) + 1) / (nsamp + 1)
if(pval_er == 1){
pval_er = (sum(t_vect_er >= t_obs_er) + 1) / (nsamp + 1)
}
par(mfrow=c(2,1), mar=c(4,4,1,2), oma=c(0,0,3,0))
plot(density(t_vect_gpp), xlab='t-value', main='', xlim = c(-8,4))
qs = quantile(t_vect_gpp, probs=c(0.025, 0.975))
dd = density(t_vect_gpp)
ddo = order(dd$x)
xdens = dd$x[ddo]
ydens = dd$y[ddo]
xdens_lt = xdens[xdens <= qs[1]]
ydens_lt = ydens[xdens <= qs[1]]
polygon(c(xdens_lt, rev(xdens_lt)), c(ydens_lt, rep(0, length(ydens_lt))),
col='lightgreen', border='lightgreen')
xdens_ut = xdens[xdens >= qs[2]]
ydens_ut = ydens[xdens >= qs[2]]
polygon(c(xdens_ut, rev(xdens_ut)), c(ydens_ut, rep(0, length(ydens_ut))),
col='lightgreen', border='lightgreen')
abline(v=t_obs_gpp, lty=2, col='red', lwd=2)
legend('topleft', legend='GPP', bty='n', text.font=2, cex=1)
legend('topleft', legend=paste('\np =', round(pval_gpp, 3)), bty='n',
text.font=1, cex=1)
plot(density(t_vect_er), xlim=c(-5, 52), xlab='t-value', main='')
qs = quantile(t_vect_er, probs=c(0.025, 0.975))
dd = density(t_vect_er)
ddo = order(dd$x)
xdens = dd$x[ddo]
ydens = dd$y[ddo]
xdens_lt = xdens[xdens <= qs[1]]
ydens_lt = ydens[xdens <= qs[1]]
polygon(c(xdens_lt, rev(xdens_lt)), c(ydens_lt, rep(0, length(ydens_lt))),
col='lightgreen', border='lightgreen')
xdens_ut = xdens[xdens >= qs[2]]
ydens_ut = ydens[xdens >= qs[2]]
polygon(c(xdens_ut, rev(xdens_ut)), c(ydens_ut, rep(0, length(ydens_ut))),
col='lightgreen', border='lightgreen')
abline(v=t_obs_er, lty=2, col='red', lwd=2)
legend('top', legend='ER', bty='n', text.font=2, cex=1)
legend('top', legend=paste('\np =', round(pval_er, 3)), bty='n',
text.font=1, cex=1)
mtext("Observed Welch's t-values (red) relative to bootstrapped null dists", 3,
outer=TRUE, line=1, font=2, cex=1.3)
# png(width=7, height=6, units='in', type='cairo', res=300,
#     filename='~/Dropbox/streampulse/figs/NHC_comparison/means_raw_boxplot.png')
par(mfrow = c(1,1))
gppHmean = paste('mean =', round(mean(gpp_68_70, na.rm=TRUE), 2))
gppCmean = paste('mean =', round(mean(gpp_new, na.rm=TRUE), 2))
erHmean = paste('mean =', round(mean(er_68_70, na.rm=TRUE), 2) * -1)
erCmean = paste('mean =', round(mean(er_new, na.rm=TRUE), 2))
boxplot(gpp_68_70, gpp_new,  -1* er_68_70, er_new,
ylab='', col='gray',
names=c('GPP 1968-70', 'GPP 2017-19', 'ER 1968-70', 'ER 2017-19'))
axis(1, at=1:4, labels=c(gppHmean, gppCmean, erHmean, erCmean),
line=1.5, col='transparent', tcl=0, font=2)
mtext(expression(paste("gm"^"-2" * " d"^"-1")), 2, line=2)
mtext('Another look at distributions, then and now (not bootstrapped)', 3,
cex=1, font=2)
